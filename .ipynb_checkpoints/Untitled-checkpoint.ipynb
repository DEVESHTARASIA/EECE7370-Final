{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc16dbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/skimage/io/manage_plugins.py:23: UserWarning: Your installed pillow version is < 8.1.2. Several security issues (CVE-2021-27921, CVE-2021-25290, CVE-2021-25291, CVE-2021-25293, and more) have been fixed in pillow 8.1.2 or higher. We recommend to upgrade this library.\n",
      "  from .collection import imread_collection_wrapper\n",
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bf650c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong_NLL(output, # Tensor of shape [b_sze, 210 (70 landmarks by 3)]\n",
    "             target, # Tensor of shape [b_sze,140]\n",
    "             ):\n",
    "\n",
    "    \"\"\"\n",
    "    The output structure is essentially a 2d tensor of of shape [b_sze, 210 (70 landmarks by 3)].\n",
    "    Each instance (210 length vector) are a sequence of (x,y) locations for 70 landmarks which makes the first 136 elements. The rest 70 elements of the vector \n",
    "    are the standard deviation of the probabilistic regression.\n",
    "    \"\"\"\n",
    "    b_sze = output.size()[0]\n",
    "\n",
    "    crit = nn.MSELoss(reduction='none')\n",
    "    loss = crit(output[:,:140].view(b_sze,2,-1), target.view(b_sze,2,-1))\n",
    "    loss = torch.sum(loss,dim=1)\n",
    "    loss = loss/(2*output[:,140:]**2)\n",
    "    \n",
    "    return torch.sum(torch.log(output[:,140:]**2)) + torch.sum(loss)\n",
    "\n",
    "\n",
    "def NLL(output, # Tensor of shape [b_sze, 210 (70 landmarks by 3)]\n",
    "             target, # Tensor of shape [b_sze,140]\n",
    "             ):\n",
    "\n",
    "    \"\"\"\n",
    "    The output structure is essentially a 2d tensor of of shape [b_sze, 210 (70 landmarks by 3)].\n",
    "    Each instance (210 length vector) are a sequence of (x,y) locations for 70 landmarks which makes the first 140 elements. The rest 70 elements of the vector \n",
    "    are the standard deviation of the probabilistic regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    b_sze = output.size()[0]\n",
    "\n",
    "    crit = nn.MSELoss(reduction='none')\n",
    "    loss = crit(output[:,:140].view(b_sze,2,-1), target.view(b_sze,2,-1))\n",
    "    loss = torch.sum(loss,dim=1)\n",
    "    loss = loss/(2*output[:,140:]**2)\n",
    "    val = torch.sum(0.5*(torch.log(output[:,140:]**2))) + torch.sum(loss)\n",
    "    val = val.float()\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "31630287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceLandmarksDataset(Dataset):\n",
    "    def __init__(self,root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.len = 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        files = os.listdir(self.root_dir)\n",
    "        self.len = int(len(files)/3)\n",
    "        return self.len\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        item_name = str(idx)\n",
    "        item_name = item_name.zfill(6)\n",
    "        img_name = self.root_dir+item_name+\".png\"\n",
    "        ldmks_file_name = self.root_dir+item_name+\"_ldmks.txt\"\n",
    "        with open(ldmks_file_name) as f:\n",
    "            landmarks = np.loadtxt(f)\n",
    "        landmarks = torch.tensor(landmarks)\n",
    "        landmarks = torch.reshape(landmarks,(140,))\n",
    "        landmarks = landmarks/256 - 1\n",
    "        #meanie = int(torch.mean(landmarks))\n",
    "        #stdie = int(torch.std(landmarks))\n",
    "        #landmarks = (landmarks-meanie)/stdie\n",
    "        \n",
    "        #landmarks = landmarks*2\n",
    "        #landmarks = landmarks - 1\n",
    "        img = Image.open(img_name)\n",
    "        resizer = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()])\n",
    "        img = resizer(img)\n",
    "        #mean, std = img.mean([1,2]), img.std([1,2])\n",
    "        #normalizing = transforms.Normalize(mean,std)\n",
    "        #img = normalizing(img)\n",
    "        #to_tensor = transforms.ToTensor()\n",
    "        #img = to_tensor(img)\n",
    "        sample = {\"image\":img,\"landmarks\":landmarks}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "69641fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FaceLandmarksDataset(\"/workspace/EECE7370-Final/Dataset/\")\n",
    "training_data,testing_data = random_split(dataset,[900,100]) \n",
    "train_dataloader = DataLoader(training_data,batch_size=16,shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data,batch_size=16,shuffle=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "237e6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = model.features\n",
    "class Mobile_LandmarkDetector(nn.Module):\n",
    "    def __init__(self,feature_extractor_model):\n",
    "            super().__init__()\n",
    "            self.feature_extractor_model = feature_extractor_model\n",
    "            self.regressor_op = nn.Sequential(nn.Dropout(p=0.4),nn.Linear(in_features=1280,out_features=210,bias=True))#,nn.BatchNorm1d(520),\n",
    "                                             #nn.ReLU(),nn.Dropout(p=0.4),\n",
    "                                             #nn.Linear(in_features=520,out_features=210,bias=False))\n",
    "          #  self.regressor_op = nn.Sequential(nn.Linear(in_features=62720,out_features=1000,bias=True),nn.ReLU(),nn.BatchNorm1d(1000),\n",
    "          #                                   nn.Dropout(p=0.4,inplace=False),\n",
    "                                        #      nn.Linear(in_features=5000,out_features=1000,bias=True),nn.ReLU(),nn.BatchNorm1d(1000),\n",
    "          #                                  nn.Linear(in_features=1000,out_features=600,bias=False),nn.Sigmoid(),nn.BatchNorm1d(600),#nn.BatchNorm1d(600),\n",
    "          #                                    nn.Dropout(p=0.4,inplace=False),\n",
    "          #                                    nn.Linear(in_features=600,out_features=210,bias=True))\n",
    "    \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.feature_extractor_model(x)\n",
    "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.regressor_op(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "our_model = Mobile_LandmarkDetector(feature_extractor_model=feature_extractor)\n",
    "#for param in our_model.feature_extractor_model.parameters():\n",
    "#    param.requires_grad = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "23ecccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e6b18686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Training loss is 689.496826171875, validation loss is 0.5990105503627232\n",
      "epoch 1\n",
      "Training loss is 690.9400634765625, validation loss is 0.5853193860735212\n",
      "epoch 2\n",
      "Training loss is 693.48388671875, validation loss is 0.6627021048409598\n",
      "epoch 3\n",
      "Training loss is 689.818359375, validation loss is 0.63000193132673\n",
      "epoch 4\n",
      "Training loss is 693.9735717773438, validation loss is 0.6291334054129465\n",
      "epoch 5\n",
      "Training loss is 690.93310546875, validation loss is 0.6508181457519531\n",
      "epoch 6\n",
      "Training loss is 693.5901489257812, validation loss is 0.5627679879324777\n",
      "epoch 7\n",
      "Training loss is 693.4259033203125, validation loss is 0.5969905809674944\n",
      "epoch 8\n",
      "Training loss is 690.3843994140625, validation loss is 0.5868107299804688\n",
      "epoch 9\n",
      "Training loss is 695.16455078125, validation loss is 0.5975168195452009\n",
      "epoch 10\n",
      "Training loss is 694.9708251953125, validation loss is 0.620442587716239\n",
      "epoch 11\n",
      "Training loss is 692.3648071289062, validation loss is 0.5952584097726004\n",
      "epoch 12\n",
      "Training loss is 691.1622314453125, validation loss is 0.6141942073277065\n",
      "epoch 13\n",
      "Training loss is 687.8087768554688, validation loss is 0.6258764692034039\n",
      "epoch 14\n",
      "Training loss is 685.9810180664062, validation loss is 0.609187029157366\n",
      "epoch 15\n",
      "Training loss is 691.420654296875, validation loss is 0.6135745718819754\n",
      "epoch 16\n",
      "Training loss is 689.22119140625, validation loss is 0.6953240400041854\n",
      "epoch 17\n",
      "Training loss is 688.9346923828125, validation loss is 0.6358713378906249\n",
      "epoch 18\n",
      "Training loss is 692.1200561523438, validation loss is 0.6046483285086496\n",
      "epoch 19\n",
      "Training loss is 694.2955322265625, validation loss is 0.5781439993722097\n",
      "epoch 20\n",
      "Training loss is 695.5337524414062, validation loss is 0.6425630711146764\n",
      "epoch 21\n",
      "Training loss is 688.1848754882812, validation loss is 0.6277254028320313\n",
      "epoch 22\n",
      "Training loss is 692.526123046875, validation loss is 0.647764879499163\n",
      "epoch 23\n",
      "Training loss is 695.1435546875, validation loss is 0.6208260323660715\n",
      "epoch 24\n",
      "Training loss is 697.1213989257812, validation loss is 0.6231426522391182\n",
      "epoch 25\n",
      "Training loss is 693.11376953125, validation loss is 0.6427322344098771\n",
      "epoch 26\n",
      "Training loss is 691.87451171875, validation loss is 0.6310203922816685\n",
      "epoch 27\n",
      "Training loss is 693.583251953125, validation loss is 0.627634765625\n",
      "epoch 28\n",
      "Training loss is 692.2047119140625, validation loss is 0.6290951843261718\n",
      "epoch 29\n",
      "Training loss is 688.7658081054688, validation loss is 0.6381961539132254\n",
      "epoch 30\n",
      "Training loss is 690.964111328125, validation loss is 0.660476824079241\n",
      "epoch 31\n",
      "Training loss is 692.3806762695312, validation loss is 0.6678299996512276\n",
      "epoch 32\n",
      "Training loss is 691.2453002929688, validation loss is 0.6384769112723214\n",
      "epoch 33\n",
      "Training loss is 693.2440795898438, validation loss is 0.6174030500139509\n",
      "epoch 34\n",
      "Training loss is 690.0748291015625, validation loss is 0.615106937953404\n",
      "epoch 35\n",
      "Training loss is 695.4661254882812, validation loss is 0.6101602478027344\n",
      "epoch 36\n",
      "Training loss is 688.3875732421875, validation loss is 0.59784226335798\n",
      "epoch 37\n",
      "Training loss is 690.0103759765625, validation loss is 0.6074992501395089\n",
      "epoch 38\n",
      "Training loss is 693.7131958007812, validation loss is 0.5848474993024553\n",
      "epoch 39\n",
      "Training loss is 694.0724487304688, validation loss is 0.6196997331891742\n",
      "epoch 40\n",
      "Training loss is 689.3292236328125, validation loss is 0.6500308794294085\n",
      "epoch 41\n",
      "Training loss is 686.9872436523438, validation loss is 0.5766447535923549\n",
      "epoch 42\n",
      "Training loss is 687.5177612304688, validation loss is 0.6085158386230469\n",
      "epoch 43\n",
      "Training loss is 690.3817749023438, validation loss is 0.6324245169503349\n",
      "epoch 44\n",
      "Training loss is 692.4935302734375, validation loss is 0.6557393798828124\n",
      "epoch 45\n",
      "Training loss is 694.472900390625, validation loss is 0.6505935843331474\n",
      "epoch 46\n",
      "Training loss is 692.9379272460938, validation loss is 0.5985849020821706\n",
      "epoch 47\n",
      "Training loss is 692.9527587890625, validation loss is 0.6113010864257812\n",
      "epoch 48\n",
      "Training loss is 694.6843872070312, validation loss is 0.6112865295410155\n",
      "epoch 49\n",
      "Training loss is 695.2503051757812, validation loss is 0.6016129760742188\n"
     ]
    }
   ],
   "source": [
    "our_model.to(device)\n",
    "optimizer = torch.optim.Adam(our_model.regressor_op.parameters())\n",
    "mse = nn.MSELoss(reduction=\"sum\")\n",
    "best_model_NLL = our_model\n",
    "best_loss = np.inf\n",
    "#Training\n",
    "for q in range(50):\n",
    "    our_model.train()\n",
    "    print(f\"epoch {q}\")\n",
    "    for i,(batch) in enumerate(train_dataloader):\n",
    "        ip,op = batch[\"image\"],batch[\"landmarks\"]\n",
    "        ip = ip.to(device)\n",
    "        op = op.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred_op = our_model(ip)\n",
    "        minie = torch.min(pred_op)\n",
    "        maxie = torch.max(pred_op)\n",
    "        pred_op = (pred_op-minie)/(maxie-minie)\n",
    "        pred_op = pred_op*2 - 1\n",
    "        #meanie = torch.mean(pred_op)\n",
    "        #stdie = torch.std(pred_op)\n",
    "        #pred_op = (pred_op - meanie)/stdie\n",
    "        #pred_op = pred_op + 1\n",
    "        #pred_op = pred_op/2\n",
    "        pred_op = F.log_softmax(pred_op,dim=1)\n",
    "        loss = NLL(pred_op,op)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar(\"Loss/train\", loss, q)\n",
    "        \n",
    "    our_model.eval()    \n",
    "    with torch.no_grad():\n",
    "        tot_loss = 0\n",
    "        for i_val,(batch_val) in enumerate(test_dataloader):\n",
    "            ip_test,op_test = batch_val[\"image\"],batch_val[\"landmarks\"]\n",
    "            ip_test = ip_test.to(device)\n",
    "            op_test = op_test.float().to(device)\n",
    "            pred_op = our_model(ip_test)\n",
    "            minie = torch.min(pred_op)\n",
    "            maxie = torch.max(pred_op)\n",
    "            pred_op = (pred_op-minie)/(maxie-minie)\n",
    "            pred_op = pred_op*2 - 1\n",
    "            writer.add_scalar(\"Loss/val\",loss,q)\n",
    "            l = mse(pred_op[:,:140],op_test)\n",
    "            tot_loss += l.item()/(70*100)\n",
    "        \n",
    "    if tot_loss < best_loss:\n",
    "        best_loss = tot_loss\n",
    "        best_model_NLL = our_model\n",
    "\n",
    "    print(f\"Training loss is {loss.item()}, validation loss is {tot_loss}\")\n",
    "\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "123afb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(-0., device='cuda:0', grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "best_model_NLL.eval()\n",
    "q = best_model_NLL(dataset[617][\"image\"].unsqueeze(0).to(device))\n",
    "minie = torch.max(q)\n",
    "maxie = torch.min(q)\n",
    "q = (q-minie)/(maxie-minie)\n",
    "print(torch.max(q))\n",
    "print(torch.min(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56bd1caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "best_model_NLL.eval()\n",
    "maxb = 0\n",
    "minb = 0\n",
    "for t in test_dataloader:\n",
    "    q = best_model_NLL.feature_extractor_model(t[\"image\"].to(device))\n",
    "#q = torch.tanh(q)\n",
    "    q = nn.functional.adaptive_avg_pool2d(q, (1, 1)) \n",
    "    maxa = int(torch.max(q))\n",
    "    mina = int(torch.min(q))\n",
    "    \n",
    "    if maxa > maxb:\n",
    "        maxb = maxa\n",
    "    if mina < minb:\n",
    "        minb = mina\n",
    "        \n",
    "print(maxb)\n",
    "print(minb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "967f5238",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([199.2880, 263.0710, 198.6850, 288.4830, 199.5730, 309.0610, 207.6470,\n",
      "        333.1010, 218.0160, 351.5190, 239.5870, 368.5540, 264.6420, 384.2490,\n",
      "        289.4260, 395.1560, 311.2890, 397.6530, 326.9350, 388.3930, 333.8160,\n",
      "        373.2060, 336.9720, 354.0820, 337.0300, 336.9720, 334.2650, 319.3750,\n",
      "        331.1890, 298.8690, 329.3120, 281.7240, 328.8930, 259.9870, 245.3160,\n",
      "        248.7480, 258.2950, 245.3060, 273.2460, 243.6810, 286.3940, 244.8460,\n",
      "        298.8000, 248.3550, 337.0580, 247.0650, 345.2810, 242.8680, 351.5970,\n",
      "        241.4370, 355.5890, 242.1830, 355.5950, 245.5710, 319.8430, 262.2910,\n",
      "        324.1780, 280.8380, 330.1780, 297.8250, 334.4750, 316.6940, 294.3200,\n",
      "        321.1610, 307.9510, 329.5800, 321.3440, 331.3090, 329.5020, 327.2070,\n",
      "        333.3070, 319.0400, 256.0030, 267.3470, 265.0080, 263.1880, 277.4720,\n",
      "        262.5520, 285.7780, 268.5940, 279.3920, 272.3360, 267.7250, 273.4420,\n",
      "        329.8760, 267.0460, 338.5210, 260.1220, 347.6640, 259.3670, 348.1340,\n",
      "        263.7310, 347.1130, 270.2390, 338.6290, 270.1680, 284.4010, 359.6880,\n",
      "        295.9740, 356.1430, 308.4460, 353.2440, 323.1490, 351.7480, 333.7460,\n",
      "        349.7370, 338.1520, 350.0470, 338.4730, 351.1400, 336.6730, 359.6550,\n",
      "        330.7640, 366.4440, 319.9300, 369.0270, 307.7000, 369.1280, 294.8620,\n",
      "        364.8960, 286.3410, 358.7830, 304.6540, 358.0030, 320.2110, 356.5360,\n",
      "        330.6630, 354.6600, 335.9250, 351.2380, 330.0000, 357.8040, 319.6190,\n",
      "        359.8290, 307.0010, 360.7290, 268.3770, 265.9600, 339.2730, 263.2600],\n",
      "       dtype=torch.float64)\n",
      "tensor(397.6530, dtype=torch.float64)\n",
      "tensor([0.0046, 0.0061, 0.0046, 0.0067, 0.0046, 0.0071, 0.0048, 0.0077, 0.0050,\n",
      "        0.0081, 0.0055, 0.0085, 0.0061, 0.0089, 0.0067, 0.0091, 0.0072, 0.0092,\n",
      "        0.0076, 0.0090, 0.0077, 0.0086, 0.0078, 0.0082, 0.0078, 0.0078, 0.0077,\n",
      "        0.0074, 0.0077, 0.0069, 0.0076, 0.0065, 0.0076, 0.0060, 0.0057, 0.0058,\n",
      "        0.0060, 0.0057, 0.0063, 0.0056, 0.0066, 0.0057, 0.0069, 0.0057, 0.0078,\n",
      "        0.0057, 0.0080, 0.0056, 0.0081, 0.0056, 0.0082, 0.0056, 0.0082, 0.0057,\n",
      "        0.0074, 0.0061, 0.0075, 0.0065, 0.0076, 0.0069, 0.0077, 0.0073, 0.0068,\n",
      "        0.0074, 0.0071, 0.0076, 0.0074, 0.0077, 0.0076, 0.0076, 0.0077, 0.0074,\n",
      "        0.0059, 0.0062, 0.0061, 0.0061, 0.0064, 0.0061, 0.0066, 0.0062, 0.0065,\n",
      "        0.0063, 0.0062, 0.0063, 0.0076, 0.0062, 0.0078, 0.0060, 0.0080, 0.0060,\n",
      "        0.0080, 0.0061, 0.0080, 0.0062, 0.0078, 0.0062, 0.0066, 0.0083, 0.0068,\n",
      "        0.0082, 0.0071, 0.0082, 0.0075, 0.0081, 0.0077, 0.0081, 0.0078, 0.0081,\n",
      "        0.0078, 0.0081, 0.0078, 0.0083, 0.0076, 0.0085, 0.0074, 0.0085, 0.0071,\n",
      "        0.0085, 0.0068, 0.0084, 0.0066, 0.0083, 0.0070, 0.0083, 0.0074, 0.0082,\n",
      "        0.0076, 0.0082, 0.0078, 0.0081, 0.0076, 0.0083, 0.0074, 0.0083, 0.0071,\n",
      "        0.0083, 0.0062, 0.0061, 0.0078, 0.0061], dtype=torch.float64)\n",
      "tensor([0.0540, 0.0712, 0.0538, 0.0781, 0.0540, 0.0837, 0.0562, 0.0902, 0.0590,\n",
      "        0.0952, 0.0649, 0.0998, 0.0717, 0.1040, 0.0784, 0.1070, 0.0843, 0.1077,\n",
      "        0.0885, 0.1052, 0.0904, 0.1011, 0.0912, 0.0959, 0.0913, 0.0912, 0.0905,\n",
      "        0.0865, 0.0897, 0.0809, 0.0892, 0.0763, 0.0891, 0.0704, 0.0664, 0.0674,\n",
      "        0.0699, 0.0664, 0.0740, 0.0660, 0.0776, 0.0663, 0.0809, 0.0672, 0.0913,\n",
      "        0.0669, 0.0935, 0.0658, 0.0952, 0.0654, 0.0963, 0.0656, 0.0963, 0.0665,\n",
      "        0.0866, 0.0710, 0.0878, 0.0760, 0.0894, 0.0806, 0.0906, 0.0858, 0.0797,\n",
      "        0.0870, 0.0834, 0.0892, 0.0870, 0.0897, 0.0892, 0.0886, 0.0903, 0.0864,\n",
      "        0.0693, 0.0724, 0.0718, 0.0713, 0.0751, 0.0711, 0.0774, 0.0727, 0.0757,\n",
      "        0.0737, 0.0725, 0.0740, 0.0893, 0.0723, 0.0917, 0.0704, 0.0941, 0.0702,\n",
      "        0.0943, 0.0714, 0.0940, 0.0732, 0.0917, 0.0732, 0.0770, 0.0974, 0.0801,\n",
      "        0.0964, 0.0835, 0.0957, 0.0875, 0.0952, 0.0904, 0.0947, 0.0916, 0.0948,\n",
      "        0.0917, 0.0951, 0.0912, 0.0974, 0.0896, 0.0992, 0.0866, 0.0999, 0.0833,\n",
      "        0.1000, 0.0798, 0.0988, 0.0775, 0.0972, 0.0825, 0.0969, 0.0867, 0.0965,\n",
      "        0.0895, 0.0960, 0.0910, 0.0951, 0.0894, 0.0969, 0.0865, 0.0974, 0.0831,\n",
      "        0.0977, 0.0727, 0.0720, 0.0919, 0.0713], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dataset = FaceLandmarksDataset(\"/workspace/EECE7370-Final/Dataset/\")\n",
    "op = dataset[6][\"landmarks\"]\n",
    "t1 = F.normalize(op, p=1.0, dim = 0)\n",
    "t2 = F.normalize(op, p=2.0, dim = 0)\n",
    "\n",
    "print(op)\n",
    "print(torch.max(op))\n",
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5b1a11d7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [78], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#our_model.to(device)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m pred_op \u001b[38;5;241m=\u001b[39m \u001b[43mour_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m pred_op \u001b[38;5;241m=\u001b[39m pred_op\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     17\u001b[0m op \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [75], line 11\u001b[0m, in \u001b[0;36mMobile_LandmarkDetector.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregressor_op(x)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(our_model.parameters(), lr=0.001)\n",
    "mse = nn.MSELoss(reduction=\"sum\")\n",
    "best_model_NLL = our_model\n",
    "best_loss = np.inf\n",
    "\n",
    "#Training\n",
    "for i in range(1):\n",
    "    our_model.train()\n",
    "    for i,batch in enumerate(train_dataloader):\n",
    "        ip,op = batch[\"image\"],batch[\"landmarks\"]\n",
    "        #ip = ip.to(device)\n",
    "        #op = op.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #our_model.to(device)\n",
    "        pred_op = our_model(ip)\n",
    "        pred_op = pred_op.float()\n",
    "        op = op.float()\n",
    "        loss = NLL(pred_op,op)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        tot_loss = 0\n",
    "        for i_val,(batch_val) in enumerate(test_dataloader):\n",
    "            ip_test,op_test = batch_val[\"image\"],batch_val[\"landmarks\"]\n",
    "            #ip_test = ip_test.to(device)\n",
    "            #op_test = op_test.float().to(device)\n",
    "            op_test = op_test.float()\n",
    "            pred_op = our_model(ip_test)\n",
    "            pred_op = pred_op.float()\n",
    "            l = mse(pred_op[:,:140],op_test)\n",
    "            tot_loss += l.item()/(68*100)\n",
    "        \n",
    "    if tot_loss < best_loss:\n",
    "        best_loss = tot_loss\n",
    "        best_model_NLL = our_model\n",
    "\n",
    "    print(f\"Training loss is {loss.item()}, validation loss is {tot_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30643bae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb0cc599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1280, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "t = model.features(dataset[6][\"image\"].unsqueeze(0).to(device))\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e3daa4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([199.2880, 263.0710, 198.6850, 288.4830, 199.5730, 309.0610, 207.6470,\n",
      "        333.1010, 218.0160, 351.5190, 239.5870, 368.5540, 264.6420, 384.2490,\n",
      "        289.4260, 395.1560, 311.2890, 397.6530, 326.9350, 388.3930, 333.8160,\n",
      "        373.2060, 336.9720, 354.0820, 337.0300, 336.9720, 334.2650, 319.3750,\n",
      "        331.1890, 298.8690, 329.3120, 281.7240, 328.8930, 259.9870, 245.3160,\n",
      "        248.7480, 258.2950, 245.3060, 273.2460, 243.6810, 286.3940, 244.8460,\n",
      "        298.8000, 248.3550, 337.0580, 247.0650, 345.2810, 242.8680, 351.5970,\n",
      "        241.4370, 355.5890, 242.1830, 355.5950, 245.5710, 319.8430, 262.2910,\n",
      "        324.1780, 280.8380, 330.1780, 297.8250, 334.4750, 316.6940, 294.3200,\n",
      "        321.1610, 307.9510, 329.5800, 321.3440, 331.3090, 329.5020, 327.2070,\n",
      "        333.3070, 319.0400, 256.0030, 267.3470, 265.0080, 263.1880, 277.4720,\n",
      "        262.5520, 285.7780, 268.5940, 279.3920, 272.3360, 267.7250, 273.4420,\n",
      "        329.8760, 267.0460, 338.5210, 260.1220, 347.6640, 259.3670, 348.1340,\n",
      "        263.7310, 347.1130, 270.2390, 338.6290, 270.1680, 284.4010, 359.6880,\n",
      "        295.9740, 356.1430, 308.4460, 353.2440, 323.1490, 351.7480, 333.7460,\n",
      "        349.7370, 338.1520, 350.0470, 338.4730, 351.1400, 336.6730, 359.6550,\n",
      "        330.7640, 366.4440, 319.9300, 369.0270, 307.7000, 369.1280, 294.8620,\n",
      "        364.8960, 286.3410, 358.7830, 304.6540, 358.0030, 320.2110, 356.5360,\n",
      "        330.6630, 354.6600, 335.9250, 351.2380, 330.0000, 357.8040, 319.6190,\n",
      "        359.8290, 307.0010, 360.7290, 268.3770, 265.9600, 339.2730, 263.2600],\n",
      "       dtype=torch.float64)\n",
      "tensor([-2.4542, -1.0265, -2.4677, -0.4576, -2.4479,  0.0030, -2.2671,  0.5411,\n",
      "        -2.0350,  0.9534, -1.5522,  1.3347, -0.9913,  1.6861, -0.4365,  1.9302,\n",
      "         0.0529,  1.9861,  0.4031,  1.7788,  0.5571,  1.4389,  0.6278,  1.0108,\n",
      "         0.6291,  0.6278,  0.5672,  0.2339,  0.4983, -0.2251,  0.4563, -0.6089,\n",
      "         0.4469, -1.0955, -1.4239, -1.3471, -1.1334, -1.4241, -0.7987, -1.4605,\n",
      "        -0.5044, -1.4344, -0.2267, -1.3559,  0.6297, -1.3848,  0.8138, -1.4787,\n",
      "         0.9552, -1.5108,  1.0445, -1.4941,  1.0447, -1.4182,  0.2444, -1.0439,\n",
      "         0.3414, -0.6288,  0.4757, -0.2485,  0.5719,  0.1739, -0.3270,  0.2739,\n",
      "        -0.0218,  0.4623,  0.2780,  0.5010,  0.4606,  0.4092,  0.5457,  0.2264,\n",
      "        -1.1847, -0.9308, -0.9831, -1.0239, -0.7041, -1.0381, -0.5182, -0.9028,\n",
      "        -0.6611, -0.8191, -0.9223, -0.7943,  0.4689, -0.9375,  0.6625, -1.0925,\n",
      "         0.8671, -1.1094,  0.8776, -1.0117,  0.8548, -0.8660,  0.6649, -0.8676,\n",
      "        -0.5490,  1.1363, -0.2899,  1.0569, -0.0108,  0.9920,  0.3184,  0.9585,\n",
      "         0.5556,  0.9135,  0.6542,  0.9205,  0.6614,  0.9449,  0.6211,  1.1355,\n",
      "         0.4888,  1.2875,  0.2463,  1.3453, -0.0275,  1.3476, -0.3148,  1.2529,\n",
      "        -0.5056,  1.1160, -0.0956,  1.0986,  0.2526,  1.0657,  0.4866,  1.0237,\n",
      "         0.6043,  0.9471,  0.4717,  1.0941,  0.2393,  1.1394, -0.0431,  1.1596,\n",
      "        -0.9077, -0.9618,  0.6793, -1.0222], dtype=torch.float64)\n",
      "tensor([-2.2153e-01,  2.7621e-02, -2.2389e-01,  1.2689e-01, -2.2042e-01,\n",
      "         2.0727e-01, -1.8888e-01,  3.0118e-01, -1.4838e-01,  3.7312e-01,\n",
      "        -6.4113e-02,  4.3966e-01,  3.3758e-02,  5.0097e-01,  1.3057e-01,\n",
      "         5.4358e-01,  2.1597e-01,  5.5333e-01,  2.7709e-01,  5.1716e-01,\n",
      "         3.0397e-01,  4.5784e-01,  3.1630e-01,  3.8313e-01,  3.1652e-01,\n",
      "         3.1630e-01,  3.0572e-01,  2.4756e-01,  2.9371e-01,  1.6746e-01,\n",
      "         2.8638e-01,  1.0048e-01,  2.8474e-01,  1.5574e-02, -4.1734e-02,\n",
      "        -2.8328e-02,  8.9648e-03, -4.1773e-02,  6.7367e-02, -4.8121e-02,\n",
      "         1.1873e-01, -4.3570e-02,  1.6719e-01, -2.9863e-02,  3.1663e-01,\n",
      "        -3.4902e-02,  3.4875e-01, -5.1297e-02,  3.7343e-01, -5.6887e-02,\n",
      "         3.8902e-01, -5.3973e-02,  3.8904e-01, -4.0738e-02,  2.4939e-01,\n",
      "         2.4574e-02,  2.6632e-01,  9.7023e-02,  2.8976e-01,  1.6338e-01,\n",
      "         3.0654e-01,  2.3709e-01,  1.4969e-01,  2.5454e-01,  2.0293e-01,\n",
      "         2.8742e-01,  2.5525e-01,  2.9418e-01,  2.8712e-01,  2.7815e-01,\n",
      "         3.0198e-01,  2.4625e-01,  1.1719e-05,  4.4324e-02,  3.5187e-02,\n",
      "         2.8078e-02,  8.3875e-02,  2.5594e-02,  1.1632e-01,  4.9195e-02,\n",
      "         9.1375e-02,  6.3813e-02,  4.5801e-02,  6.8133e-02,  2.8858e-01,\n",
      "         4.3148e-02,  3.2235e-01,  1.6102e-02,  3.5806e-01,  1.3152e-02,\n",
      "         3.5990e-01,  3.0199e-02,  3.5591e-01,  5.5621e-02,  3.2277e-01,\n",
      "         5.5344e-02,  1.1094e-01,  4.0503e-01,  1.5615e-01,  3.9118e-01,\n",
      "         2.0487e-01,  3.7986e-01,  2.6230e-01,  3.7402e-01,  3.0370e-01,\n",
      "         3.6616e-01,  3.2091e-01,  3.6737e-01,  3.2216e-01,  3.7164e-01,\n",
      "         3.1513e-01,  4.0490e-01,  2.9205e-01,  4.3142e-01,  2.4973e-01,\n",
      "         4.4151e-01,  2.0195e-01,  4.4191e-01,  1.5180e-01,  4.2538e-01,\n",
      "         1.1852e-01,  4.0150e-01,  1.9005e-01,  3.9845e-01,  2.5082e-01,\n",
      "         3.9272e-01,  2.9165e-01,  3.8539e-01,  3.1221e-01,  3.7202e-01,\n",
      "         2.8906e-01,  3.9767e-01,  2.4851e-01,  4.0558e-01,  1.9922e-01,\n",
      "         4.0910e-01,  4.8348e-02,  3.8906e-02,  3.2529e-01,  2.8359e-02],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "t = dataset[6][\"landmarks\"]\n",
    "meanie = torch.mean(t)\n",
    "stdie = torch.std(t)\n",
    "q = (t-meanie)/stdie\n",
    "\n",
    "z = t/256 - 1\n",
    "\n",
    "print(t)\n",
    "print(q)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "76a4e668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, Dropout(p=0.4, inplace=False)), (1, Linear(in_features=327680, out_features=210, bias=True)), (2, Sigmoid())]\n"
     ]
    }
   ],
   "source": [
    "our_model.to(device)\n",
    "our_model.eval()\n",
    "#print(our_model(dataset[6][\"image\"].to(device).unsqueeze(0)))\n",
    "#print(torch.min(dataset[344][\"image\"]))\n",
    "features = our_model.feature_extractor_model(dataset[6][\"image\"].to(device).unsqueeze(0))\n",
    "features = torch.flatten(features,1)\n",
    "children = [i for i in our_model.regressor_op.children()]\n",
    "listed = [(i,j) for (i,j) in enumerate(children)]\n",
    "print(listed)\n",
    "for i,j in enumerate(children):\n",
    "    if i < 6:\n",
    "        features = j(features)\n",
    "    if i == 6:\n",
    "        print(features)\n",
    "        features = j(features)\n",
    "        print(features)\n",
    "    if i == 7:\n",
    "        features = j(features)\n",
    "        print(features)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c56527485b7c9dd36c8c8808345694ae9d0a642c0f00dfb6356f9a65494ceb50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
